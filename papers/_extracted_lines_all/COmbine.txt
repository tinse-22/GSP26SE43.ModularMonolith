===== PAGE 1 =====
Combining Static and Dynamic Approaches for Mining and
Testing Constraints for RESTful API Testing
‚àó ‚Ä†
Hieu Huynh Vu Nguyen Tien N. Nguyen
‚àó
University of Science, VNU-HCM University of Texas at Dallas
Tri Le
Katalon Inc. Dallas, Texas, USA
minhhieu2214@gmail.com
Ho Chi Minh city, Vietnam tien.n.nguyen@utdallas.edu
lqtri691@gmail.com
nvu@fit.hcmus.edu.vn
Katalon Inc.
Ho Chi Minh city, Vietnam
Abstract ensures the correctness of the response data by checking it against
a specified schema. This involves verifying the presence of all re-
In API testing, deriving logical constraints on API response bodies
quired properties and ensuring the data types of these properties
is crucial in generating the test cases to cover various aspects of
match their schema in the specification.
RESTful APIs. However, existing approaches are limited to dynamic
While status code and schema validation effectively cover aspects
analysis in which constraints are extracted from the execution of
of data representation and status checking, they may overlook the
APIs as part of the system under test. The key limitation of such a
logical correctness and validity of the response data from the APIs ,
dynamic approach is its under-estimation in which inputs in API
which is essential for software reliability. For example, an API
executions are not sufficiently diverse to uncover actual constraints
request for a customer older than 18 receiving a response for one
on API response bodies. In this paper, we propose to combine a
younger than 18 would not be detected by just validating the status
novel static analysis approach (in which the constraints for API re-
code or schema. Deriving logical constraints on API response bodies
sponse bodies are mined from API specifications), with the dynamic
is essential for generating test cases to cover RESTful APIs.
approach (which relies on API execution data). We leverage large
The state-of-the-art approaches for mining constraints on API
language models (LLMs) to comprehend the API specifications,
response bodies focus only on dynamic analysis in which the con-
mine constraints for response bodies, and generate test cases. To re-
straints are extracted from the execution data of the system under
duce LLMs‚Äô hallucination, we apply an Observation-Confirmation
test (SUT). AGORA [ 9 ] automatically detects invariants ‚Äîproperties
(OC) scheme which uses initial prompts to contextualize constraints.
that should consistently hold true. To identify invariants, which serve
Our empirical results show that LLMs with OC prompting achieve
as logical constraints on API response bodies, it extends Daikon [ 16 ],
high precision in constraint mining with the average of 91.2%. When
a dynamic instrumenter used to detect invariants during execution.
combining static and dynamic analysis, our tool, RBCTest , achieves
As with dynamic analysis, it under-estimates the constraints due to
a precision of 78.5%. RBCTest detects 107 constraints that the dy-
the lack of diverse inputs to cover different aspects of API response
namic approach misses and 46 more precise constraints. We also use
bodies. For example, the inputs of the APIs might not be diverse
its generated test cases to detect 21 mismatches between the API
enough to discover that the minimum age for an operation on a
specification and actual response data for 8 real-world APIs. Four
website is 18. Another limitation is that it requires the SUT operate
of the mismatches were, in fact, reported in developers‚Äô forums.
accurately to extract the constraints from inputs and outputs.
We propose RBCTest , a combined approach between a novel
1 Introduction
LLM-based static approach to mine the constraints of API response
By adhering to the principles of Representational State Transfer
bodies from the API specification, and the dynamic approach as
(REST), the RESTful APIs provide a standardized way for interop-
in AGORA [ 9 ]. Both approaches complement to each other in the
erability among components and software systems. RESTful API
following ways. First, constraint mining remains feasible even if
testing helps identify and resolve several issues, ensuring that APIs
only the API specification or execution data is available. For in-
perform as expected [ 8 , 10 , 14 , 24 , 27 , 33 ]. It also helps verify that
arXiv:2504.17287v1 [cs.SE] 24 Apr 2025
stance, when an API specification exists but the APIs and the sys-
APIs adhere to specifications and handle edge cases gracefully.
tem utilizing them are still under testing and development and may
Among techniques for API testing, black-box testing uses the Ope-
not function correctly. Conversely, in a regression testing scenario
nAPI Specification (OAS) as a basis to generate test cases and data
without up-to-date specification, the SUT with APIs from previous
[ 18 , 22 , 27 ]. The state-of-the-art API testing approaches are fo-
versions has been functioning, while the current version is still un-
cused on status code [ 5 , 6 , 22 , 33 ] and schema validation [ 5 , 6 , 33 ],
der development. In such cases, execution data from the previous
even with rule extraction using human-readable descriptions in the
version can be used to extract constraints to generate test cases for
OAS [ 19 ]. In status code validation , each HTTP request returns a
regression testing. Second, the specification often provides API de-
response with a status code, a three-digit integer, indicating the
tails, including request bodies sent to the API and response bodies
outcome of the HTTP request. Current testing approaches define
returned for each operation. This enables mining constraints on the
an oracle for a test case by validating whether the response status
API‚Äôs response bodies to uncover more comprehensive information
code matches the expected value. In contrast, schema validation
about these constraints, which are overlooked by the dynamic ap-
‚àó
Both authors contributed equally to this research.
proach. In contrast, due to actual execution, the runtime data helps
‚Ä†
Corresponding author.
derive more detailed constraints not defined in the specifications.
1

===== PAGE 2 =====
Submitted to a conference, Mar 15, 2025, XXX, XXX Hieu Huynh, Tri Le, Vu Nguyen, and Tien N. Nguyen
To mine constraints, we leverage the ability of large language
1 (a) charge:
models (LLMs) to comprehend natural language descriptions in API
2 description : The ' charge ' object represents an attempt to move
money into your account.
specifications. Constraints on API response bodies are inferred from
3 properties :
4 amount:
different sources, including response properties, response schema,
5 description : A positive integer can be up to eight digits .
operations, and request parameters. We also harness LLMs‚Äô profi- 6 type : integer
7 example: 99999999
ciency to create test cases from the mined constraints to verify if the
8 created :
9 description : Time at which the object was created . Measured
SUT correctly returns the content satisfying the mined constraints.
in seconds since the Unix epoch.
10 type : integer
Moreover, we apply an Observation-Confirmation scheme. We di-
11 currency:
vide the task of mining constraints into two phases: observation
12 description : Three lowercase letters .
13 type : string
and confirmation. The initial prompt contextualizes the description
14 example: usd
15 customer:
of constraints, enabling the next prompt to more accurately decide
16 description : ID of the customer this charge is for if existed .
their presence. As another issue in LLMs‚Äô exploration capability, 17 type : string ...
they could produce resulting constraints that are not true. Thus,
we enhance RBCTest with two extra mechanisms. First, before
requesting the LLM to make observations concerning constraints
1 (b)paths :
2 /v1/charges :
on parameters, we perform a filtering process to keep only the valid
3 get :
4 description : Returns a list of charges you have created . The
ones. Second, after generating test cases for mined constraints, we
charges are returned in sorted order ...
5 parameters:
add a semantic verifier to verify those test cases against the exam-
6 name: created
ples specified in the OAS file. The idea is that such examples tend to
7 description : Only return charges that were created during the
given date interval .
be correct because they illustrate the descriptions on the data types
8 schema:
9 anyOf:
or the data. For example, the OAS could give "March" as a valid
10 ‚àí properties :
month. If such a correct example does not pass a test case generated
11 gt ( integer )
12 lt ( integer )
by RBCTest based on the mined constraint(s), the test case must
13 name: customer
14 description : Only return charges for the customer specified by
be incorrect, which is caused by incorrect constraint(s). Thus, our
this customer ID.
verifier will discard them, leading to an improved precision.
15 schema:
16 type : string ...
We evaluated RBCTest on two datasets, one from the baseline
[ 9 ] (AGORA dataset) with 11 operations in 7 API services and our
Figure 1: (a) Schema of a response for ‚Äòcharge‚Äô API in the
RBCTest dataset collected from 8 real-world API services consisting
project Stripe described in a OAS file and (b) a simplified
of 59 endpoints and 83 operations [ 7 ]. Our empirical results show
description for the GET charges API operation from Stripe.
that GPT-4-turbo and our OC prompting achieves high precision in
constraint mining with the average of 91.2% . RBCTest achieves a
precision of 78.5% , and detects 107 constraints that the dynamic 1 {
2 "id": "ch...15",
approach misses and 46 more precise constraints . We also 3 "object": "charge",
4 "customer": "cus_id",
leverage its generated tests to detect the mismatches between the 5 "amount": 1099,
6 "created": 1679090539,
API specifications and actual execution of the SUTs. A detected 7 "currency": "usd",...
8 }
mismatch indicates a fault in the SUT or that the specification does
not reflect the SUT. We report 21 actual faults found in 8 real-world
Figure 2: A response‚Äôs body from a GET request for Stripe.
applications, including 4 issues reported by users on GitLab Forum
[1‚Äì4]. In brief, this paper makes the following contributions:
representing the lower and upper bounds of the time range, respec-
1. RBCTest : [A combination of static and dynamic ap-
tively. Users can also specify the customer for whom they wish to
proaches] for constraint mining and test generation for API re-
retrieve charging history (lines 13-16). A successful request returns
sponse bodies by using API specifications.
a response with a status code of 200 and a response body with
2. [A manually-verified benchmark] for API response bodies
data. The structure of the response data is outlined in the schema in
is available [7] for future research on API testing approaches.
Figure 1(a), which consists of a list of charge objects, each associ-
3. [An extensive evaluation] showing RBCTest outperform-
ated with various properties, e.g., amount, created timestamp, cur-
ing the state-of-the-art baselines.
rency, and others. For instance, a GET request to the /v1/charges end-
2 Motivating Example
point with parameters like ‚Äòcreated[gt]=1679090500&customer=cus_idA‚Äô
returns a list of charges that satisfy the given conditions. An ex-
2.1 Example and Observations
ample response object is displayed in Figure 2. The content of the
To illustrate the challenges and motivate our approach, we use
response is constrained by the actual input parameters in the re-
Stripe, an online payment service, streamlining the process of charg-
quest. Thus, a complete testing process needs to verify the response
ing customers via APIs. Figure 1(b) shows a simplified description
content in addition to the returned status code. For example, a test
from the API specification (OAS), detailing the GET operation for
case can check if the created field of each returned charge object
retrieving past charges. This API enables users to retrieve charging
falls within the specified interval and ensuring that the customer
records within a specific time interval (lines 11-12). The time inter-
field matches the requested ID.
val is defined by the gt (greater than) and lt (less than) parameters,
2

===== PAGE 3 =====
Combining Static and Dynamic Approaches for Mining and Testing Constraints for RESTful API Testing Submitted to a conference, Mar 15, 2025, XXX, XXX
Observation 1 (Constraints from input parameters). The on the values observed during the execution, which might not be
response data is constrained by the input parameters from the request. diverse enough to reveal the correct constraints. For example, for
When testing, in addition to verifying the status code, testers need to all the inputs, the charge values might never reach 99,999,999, thus,
verify the response data. Daikon returns the maximum charge that is less than that value.
In Figure 1(a), the descriptions on the properties of the returned
2.3 Key Ideas
charge objects define certain constraints on the attributes of those
From the above observations, we draw the following key ideas
objects. For example, the amount property must be a positive integer,
to design RBCTest , an approach for mining the constraints of
with a maximum value of eight digits. The currency attribute has a
response bodies and then generating test cases to validate them.
three-letter lowercase code (e.g., usd ).
Key Idea 1 [Combining Static and Dynamic Approaches to
Observation 2 (Constraints within response body). Natural
Mine Constraints of API‚Äôs response bodies]. The first component
language descriptions on operations express logical constraints on
of RBCTest is a novel LLM-based static approach to mine the con-
operations, their responses and others, formatting requirements, or
straints for API‚Äôs response bodies from the OpenAPI Specification
value range limitations that must be validated during API testing.
(OAS). The constraints on the response data can be found in the
The OAS file often includes examples that illustrate specific
specification in either the descriptions of the operations (e.g., line 5
constraints on data or data types. For instance, in Figure 1(a), the
of Figure 1(a)) or the descriptions of the schema for the response data
file provides an example of $999,999.99 as a positive number for
(lines 2‚Äì24 in Figure 1(b)). For example, the description in Figure 1
the property amount . A mined constraint that does not match its
states that ‚Äòthe charges are returned in a sorted order with the most
corresponding example may be incorrect. To validate that, we utilize
recent ones appearing first‚Äô . The schema for the returned values
the generated test case for the constraint.
in Figure 1 also provides us several constraints on the parameters
(lines 5‚Äì6) as well as the description of the returned object as a
Observation 3 (Verification with examples). The illustrating
whole (line 2). For example, the amount of a charge is a positive
examples in the description can be used to verify against the generated
number with up to 8 digits and such value must be of integer.
test cases, i.e., the validity of the mined constraints.
We integrate AGORA as the dynamic component, leveraging
execution data from the SUT to mine invariants for API response
2.2 State-of-the-art Approaches
bodies. The static and dynamic components complement each other.
In API test case generation, validation typically falls into two main
First, constraint mining remains feasible even if only the API speci-
categories: status code validation and schema validation.
fication or execution data is available. Second, the static component
Status Code Validation: Each HTTP request is returned with
facilitates mining constraints on API response bodies from API
a response containing a status code and data. The status code, a
specifications, capturing broader constraint information that the
3-digit integer, indicates the outcome of the HTTP request. 2xx
dynamic approach might miss. Conversely, runtime data, derived
codes signify a successful request. Conversely, 4xx codes indicate
from actual executions, can help refine constraints, providing more
errors, such as a bad syntax request or invalid input values. For
precise insights than those typically found in API specifications.
instance, in testing the API ‚ÄôGET/user_information‚Äô with various valid
Key Idea 2 [Observation-Confirmation scheme on LLMs for
and invalid user IDs, testers would expect the API to return status
constraints discovery and test generation] . For the task of ex-
codes 200 or 404 based on the inputs provided. This validation
tracting constraints from API specifications, we utilize the ability
method is widely used in automation testing tools, e.g., Postman [ 6 ]
of LLMs to comprehend natural language descriptions found in API
and Katalon [5], RestTestGen [33], or KAT [22].
specifications for constraint mining on the APIs and their parame-
Schema Validation: Schema validation ensures the response
ters. Our experiment showed that direct use of LLMs for constraint
correctness by checking it against a predefined response schema.
mining yields sub-optimal performance. To improve it, we apply
This verifies the presence of all required properties and the con-
an Observation-Confirmation scheme in which the initial result
sistency of property data types with their specifications. A lack of
returned from the LLMs will be fed back to themselves in a confir-
required data or a mismatch in data types indicates errors in API
mation prompt to provide better contexts on the constraints.
services. Tools, e.g., RestTestGen [ 33 ], leverage external libraries,
Key Idea 3 [Generating test cases for the constraints on re-
such as ‚Äòswagger-schema-validator‚Äô, to facilitate schema validation.
sponse bodies]. Our goal is to advance beyond current API testing
While status code and schema validation effectively cover aspects
techniques: we also generate test cases to evaluate the mined con-
of data representation and status checking, they may overlook the
straints. For instance, a test case is generated to verify that the list
logical correctness and validity in the response data . For instance, if
of charges returned by the API endpoint ‚Äòv1/charges‚Äô is sorted in
an API request for a charge in 2025 returns one from 2024, or if the
reverse chronological order. Another example includes generating a
charge amount is negative, these issues would not be detected by
test case to validate the format/value of the returned charge amount.
merely validating the status code or schema.
Key Idea 4 [Filter and Semantic Verifier]. Before asking the
To address that, the state-of-the-art dynamic approach, AGORA [ 9 ]
LLM to analyze constraints on parameters, we first perform a fil-
extends Daikon [ 16 ], a dynamic instrumenter, to infer the invariants
tering process to remove invalid constraints. After generating test
from the values extracted from the execution of the SUT using the
cases based on the mined constraints, we then introduce a semantic
APIs. AGORA considers these derived invariants as the constraints
verifier to check these test cases against the examples in the OAS.
for the response bodies. However, inherent from the nature of a
The rationale is that these examples are typically accurate, as they
dynamic approach, the quality of the derived invariants depends
3

===== PAGE 4 =====
Submitted to a conference, Mar 15, 2025, XXX, XXX Hieu Huynh, Tri Le, Vu Nguyen, and Tien N. Nguyen
1
LLM
Operation Description
Operation
Operation Observation
Operation
Description
description
Operation
observation
Extraction
observing instruction
2
5
LLM LLM
4
Request Parameter
LLM Request-Response
Request-Response
for
Request
Request-
- param 1 Observation Constraint Confirmation
each
- param 1 Parameter Mapping
Parameter
- param A
Parameters
Response
- descr 1 Parameter
Request-response
- descr 1
observation Request-response parameter
- descr A
Extraction constraint
observing instruction
constraint confirming instruction
mapping instruction
OpenAPI
yes/no
3
LLM
Spec Response Schema
- param 1
Observation
- param 1
Schema
- prop X
- descr 1
- descr 1 Schema
- descr X observation
observing instruction
Request-Response
Constraints
Extract Constraints from Request Specification and Operation Description
Syntactic
Filter
Extract Constraints from Response Specification
Response Property
6
7 Constraints
LLM LLM
Property
Constraint
Property
Observation
- param 1
- param 1 for
- param 1 Confirmation
Property
- param 1
- prop X Detected Constraints
Extraction
- descr 1
- descr 1 each
Property
- descr 1
- descr 1 observation Constraint confirming
- descr X
yes/no
observing instruction
instruction
Figure 3: Static Constraint Mining with LLMs
represent the valid data types or values. For instance, an example
1 PARAMETER_SCHEMA_MAPPING_PROMPT = ''' Given a request
of $999,999.99 is used to illustrate a positive number for amount . We
parameter and an API response schema, check if there is
a matching property in the API response schema.
can validate the generated test cases against such examples. If a
2 Request parameter for {method} - {endpoint}:
3 {parameter}: {description}
test case fails to validate a given correct example, it suggests that
4
5 Follow these steps to find the matching property: {
the mined constraint may be incorrect.
Instructions for chain-of-thought steps}
6
7 Schema specification {schema}: {schema_observation}
3 Static Constraint Mining
8 Confirm if the request parameter has a matching property in
the response schema: ...
To mine the constraints, we observe that the specification for an
9 Identify the corresponding property name of the provided
request parameter in the schema. ...
API endpoint includes two main parts: the request specification (the
10 If a matching property exists, explain it using this format:
input of the API) and response schema specification (the output of ... '''
the API). (1) A request specification determines how to call an API
endpoint, detailing the required input parameters , their roles, and
the parameters within the request body along with their respective
Figure 4: Request-Response Parameter Mapping Observation
roles. It might also contain the description of the API operations .
(2) The response schema specification provides instructions on the For instance, to verify a "customerID" constraint from Observation
response data , including each property in the response data, its 1, the response data should contain a property that identifies the
description, datatype, nullability, and other relevant details. These customer. Thus, the required pair is <"customerID", "customer">. If
parts contain descriptions that are the targets for constraint mining. the response data does not have a field to represent a constraint,
that constraint is disregarded as it cannot be validated.
3.1 Constraints from Request Specifications
An essential component of an API specification is the request spec-
1 MAPPING_CONFIRMATION = ''' {System prompt}
ification. This part instructs clients on how to correctly initiate
2 The request parameter ' s information:
3 - Operation: {method}
API calls, detailing the required inputs and the returned data. Our
4 - Parameter: {parameter_name}
5 - Description: {description}
constraint mining relies on natural language descriptions attached
6 The corresponding property ' s information:
7 - Resource: {schema}
to request parameters or response properties. Since descriptions are
8 - Corresponding property: {corresponding_property}
optional in OAS, they may appear in one endpoint but not others.
9 {Instructions for chain-of-thought steps}
10 Answer format: ... '''
To extract them, we first check the current response schema; if none
are found, we search the entire OAS document. Unlike KAT [ 22 ],
Figure 5: Request-Response Constraint Confirmation
which uses LLM-based "description mapping," we avoid potential
inaccuracies that could propagate errors. Instead, we adopt descrip-
tions from properties with the same name in other schemas. If none
3.1.2 Detailed Process. The process of extracting constraints from
exist, we exclude the property, prioritizing precision over recall.
request parameters encompasses four steps: (1) description extrac-
tion, (2) observation ( 1 - 3 from Figure 3), (3) Request-Response
3.1.1 Request-Response Constraint Mapping. The idea is that to
constraint mapping ( 4 ), and (4) constraint confirmation ( 5 ).
determine a constraint from request parameters, the response data
Initially, the description extraction phase follows the process
must contain a property that reflects this constraint. Thus, we iden-
in Algorithm 1. An API request comprises multiple parameters
tify pairs consisting of a request parameter that includes a constraint
and a response data schema. First, a prompt is made to gather
and a response data property that reflects this constraint (Figure 4).
observations on the response schema and operations associated
4

===== PAGE 5 =====
Combining Static and Dynamic Approaches for Mining and Testing Constraints for RESTful API Testing Submitted to a conference, Mar 15, 2025, XXX, XXX
Algorithm 1 Extract Constraints from Request Parameters Algorithm 2 Extract Constraints from Response Specifications
Input: API_spec(dict): obj of entire API specification. Input: API_spec(dict): obj of entire API specification.
req_spec(dict): specification obj of a certain request. response_schema(dict): schema obj of a certain response.
resp_schema(dict): schema obj of the associated response. knowledge_base(dict): gained knowledge.
Output: list of request-response constraint properties Output: list of constraint properties.
1: function getConstraintInsideResponseSchema(API_spec, re-
1: function getConstrFromReqParas(API_spec, req_spec, resp_schema)
sponse_schema, knowledge_base)
2: reqRespConstraints ‚Üê []
2: constraint_properties ‚Üê []
3: respSchemaObser ‚Üê LLM().respSchemaObser(resp_schema)
3: for each prop in response_schema do
4: operationObservation ‚Üê LLM().operationObser(desc)
4: desc ‚Üê response_schema[prop]["description"]
5: for each param in req_spec do
5: if desc = NULL then
6: desc ‚Üê req_spec[param]["desc"]
6: desc ‚Üê exactMatchProp(API_spec, prop)
7: if req_spec[param]["desc"] = NULL then
7: if desc = NULL then
8: desc ‚Üê findExactMatchParameter(API_spec, param)
8: continue # Skip this property
9: if desc = NULL then
9: if prop in knowledge_base then
10: continue # Skip this param
10: if knowledge_base[prop] = TRUE then
11: # Use LLM to find constraint for this param
11: constraint_properties.append(prop)
12: paramObser ‚Üê LLM().parameterObser(param, desc)
12: else
13: answer, corrProp, explain ‚Üê LLM().reqRespMapping
13: datatype ‚Üê response_schema[prop]["datatype"]
(param, desc, paramObser, respSchemaObser)
14: prop_obser ‚Üê LLM().propertyObser(prop, datatype, desc)
14: if answer = TRUE then
15: constraint_confirmation ‚Üê LLM().constraint_confirmation
15: confirmation ‚Üê LLM().confirmReqRespMapping
(prop, datatype, desc, prop_obser)
(param, corrProp, explain)
16: if constraint_confirmation = TRUE then
16: if confirmation = TRUE then
17: constraint_properties.append(prop)
17: reqRespConstraints.append((param, corrProp))
18: # Add this property to knowledge base
18: return reqRespConstraints
19: knowledge_base[prop] ‚Üê constraint_confirmation
19: end function
20: return constraint_properties
21: end function
with this request (lines 3‚Äì4, Algorithm 1). For a request parameter,
we engage the LLM to provide its observations on that parameter
3.2 Constraints from Response Specifications
by presenting it alongside its description (line 12, Algorithm 1). The
We examine descriptions within the response schema specifica-
LLM is expected to provide a description of a constraint within this
tion, as they provide direct constraints by mapping each property.
parameter description. If the description is missing, the specification
Each endpoint includes a response schema that guides clients on
is searched for another parameter with an identical name.
parsing returned data, structuring the response object with proper-
These descriptions support the next step: Request-Response pa-
ties and descriptions (Figure 2). Our constraint-mining algorithm,
rameter mapping (Block 4 , line 13). We guide the LLM through a
leveraging LLM, is outlined in Algorithm 2. It first extracts de-
two-step reasoning process using a tailored prompt (Figure 4). The
scriptions for each property (lines 4-8) following Section 3.1. If a
LLM receives a brief description of the request parameter, including
description exists, we check a knowledge base of LLM-identified
its details and observations ( 2 ), ensuring a clear understanding of
constraints to avoid redundant queries. If found, we reuse the stored
its intent and constraints. Second, we present the response schema,
information; otherwise, we prompt LLM to extract constraints.
observations from Block 3 , and ask the LLM to identify a match-
Observation-Confirmation Strategy . This process involves
ing property. A match occurs when the request parameter filters
two phases: observation (line 14) and confirmation (line 15). Our
response data or both share the same value meaning.
experiments reveal that when descriptions lack detail for constraint
From this two-step reasoning, we require the LLM to answer
extraction, LLM may resort to fabricating details, a phenomenon
three questions: (1) Is there a matching property in the response
known as hallucination. Drawing inspiration from the Chain-of-
schema? (2) What is this property? (3) How does the request pa-
thought [ 34 ], we divide the task of extracting constraints into two
rameter influence this property?
phases of observation and confirmation , the initial prompt better
If the answer to (1) is false, indicating that no property reflects
contextualizes the description of constraints, enabling the subse-
this request parameter, we disregard this parameter. Otherwise, we
quent prompt to more accurately determine a constraint.
proceed to the final prompt (Figure 5), which is the confirmation
In the observation phase (Block 6 ), LLM is prompted to identify
of the mapping (lines 14‚Äì17, Block 5 ). In this prompt, we present
constraints from the description. For example, given a property date
the pair of the request parameter and the matched property from
(type: string ) described as ‚ÄúISO date: the literal date of the holiday‚Äù ,
Block 4 and ask the LLM to confirm the accuracy of this mapping
LLM might infer: "The date must follow the YYYY-MM-DD format for
(line 15). This step aims to minimize the occurrence of false pos-
validity, ensuring consistency within the API.‚Äù This adds specificity
itives. Finally, the validated pairs of the request parameters and
not explicitly mentioned in the description.
the response properties are stored as Request-Response constraints
Next, the observation is fed into the Constraint Confirmation
(lines 16‚Äì17).
Prompt (Block 7 ), where the LLM validates whether the extracted
constraint provides enough detail for script generation. This step,
5

===== PAGE 6 =====
Submitted to a conference, Mar 15, 2025, XXX, XXX Hieu Huynh, Tri Le, Vu Nguyen, and Tien N. Nguyen
1 CONSTRAINT_TEST_GEN_PROMPT = '''
Figure 6: Example outputs from RBCTest and AGORA.
2 Generate a Python script to check if a property in a REST
API ' s response meets specified constraints and rules.
ID Constraints Invariants
3 Constraint description:
the number of returned items
4 - Constraint from request parameter: {parameter}
1 has to be less than or equal input.limit >= size(return.items[])
5 - Constraint description: {constraint_description}
6 API response schema: {response_schema_specification}
to the requested limit
7 The property of the provided request parameter in API
2 ‚Äì return.total >= size(return.items[])
response:
3 return.total is an integer larger return.total >= 1
8 - "{property}": "{prop_description}"
4 than or equal to 1 return.total is Integer
9 Based on the provided constraint from request param, and the
respective attribute in the API response, generate a
5 ‚Äì input.market is a substring of return.href
Python script to verify the ' {property} ' property in
number of adult
the response.
6 return.adults is Integer
guests (1-9) per room
10 Rules: {Rules for test gen}
11 Format the script as shown: ... '''
return.price must be
7 ‚Äì
within input.price_range
Figure 7: Constraint Test Generation Prompts
8 An amount is a positive integer ‚Äì
can be up to eight digits ‚Äì
Response
Body
similar to Figure 5, ensures that constraints specify values, ranges,
matched/
Execute
mismatched/
Request-Response Test Cases
or formats. If confirmed, the constraint is marked as a Response
LLM
unknown
for Constraints Test
Constraints
each Generation
Constraint Semantic
Property constraint and stored in the knowledge base.
Constraint test
Verifier
Response property Test Cases
generation instruction
constraints
Detected Constraints
4 Combining Constraints and Invariants
Figure 8: Constraint Test Generation in RBCTest
Constraints detected by our static method are based on OAS while
invariants determined by AGORA come from execution data. We
present an ensemble approach to combine constraints and invari-
confirms that the response satisfies the constraint, whereas ‚Äòmis-
ants. Figure 6 shows sample outputs produced by each method.
matched‚Äô indicates a violation. An ‚Äòunknown‚Äô outcome suggests
Several constraints can only be identified using OAS. For instance,
the absence of the relevant property, possibly due to its optional
Constraint 8 in Figure 6, which specifies that the charge amount
nature in the specification. We also incorporate a semantic verifier
must be smaller than 99,999,999, can only be extracted from the
that cross-checks test cases against examples in the OAS file. If a
OAS. This is because AGORA requires a sufficiently large num-
test case fails to validate a given correct example, it suggests that
ber of API executions to infer such constraints. Conversely, some
the mined constraint may be incorrect.
constraints can only be inferred at run-time, e.g., those related to
returned Hrefs‚ÄîURLs containing requested information.
5.1 Request-Response Constraints Testing
In AGORA, each detected invariant is associated with a set of
The Request-Response Constraint Test identifies dependencies be-
variables, as depicted in Figure 6, and a given set of variables may
tween a constrained request parameter and its corresponding re-
be linked to one or more invariants (e.g. Invariants 3-4). In our
sponse property. To generate a test case, we use the prompt in Fig-
static method, each constraint is tied to a specific set of variables,
ure 7, which requires four inputs: the parameter name, its constraint
which aligns with how AGORA groups invariants. Because each
description, the corresponding property name, and the response
constraint is modeled via textual representations, we leverage an
schema. LLM generates a validation function with two inputs: the
LLM and its natural-language text understanding to derive the
response body and the request parameter . It then creates a script to
relevant variables in the constraint.
check conditions between them. For instance, when validating a
If a constraint and an invariant involve different sets of variables,
‚Äòcreated‚Äô time interval (Observation 1), LLM extracts the ‚Äòcreated‚Äô
we include both in the resulting constraints for RBCTest , as they
time from the response body and the conditional values from the
are uniquely detected by each approach. If they involve the same,
request parameter (‚Äòcreated[gte]‚Äô, ‚Äòcreated[lte]‚Äô) before performing
we select the constraint or invariant with the stricter condition. For
logical comparisons. To ensure robustness, we guide LLM with pre-
instance, in Figure 6, row 6, we chose the constraint identified by
defined rules: using a try-catch block for error handling, excluding
our static method, as it is stricter than the invariant detected by
examples, and following a standardized function template. These
AGORA. To determine which is stricter, we leverage an LLM, which
rules ensure consistency and focus on constraint verification.
can interpret well the conditions expressed in natural language.
5.2 Response Property Constraints Testing
5 Constraint Test Generation
The Response Property constraint Test directly correlates the prop-
Constraint tests are generated using LLM (Figure 8) based on
erty‚Äôs description with the property itself. To create a test case for
two types of constraints: Request-Response constraints and Re-
this constraint, we utilize a specific prompt (Figure 7). This prompt
sponse Property constraints. Request-Response constraints stem
requires three inputs: the property name, the constraint description,
from request parameter descriptions, while Response Property con-
and the response data schema. The description extracted from the
straints are derived from the response schema. These tests take a
previous mining step gives the necessary information for gener-
response body and request details as input, producing outcomes
ating constraint validation code, while the response data schema
of ‚Äòmatched,‚Äô ‚Äòmismatched,‚Äô or ‚Äòunknown.‚Äô A ‚Äòmatched‚Äô outcome
defines the structure and type of the expected data. This schema
guides LLM in generating code to parse response data. We guide
6

===== PAGE 7 =====
Combining Static and Dynamic Approaches for Mining and Testing Constraints for RESTful API Testing Submitted to a conference, Mar 15, 2025, XXX, XXX
Table 1: Constraints Test Generation on AGORA dataset.
7 Experimental Results
Static Dynamic Unique Overlapping
7.1 Constraints Test Generation Accuracy (RQ1)
APIs Total TP P Total TP P Static Dynamic +S +D Eq.
7.1.1 Methodology. We ran our LLM-based static method to detect
A.Hotel 44 39 88.6 117 61 52.1 20 34 8 1 10
GitHub‚Äô 16 13 81.3 198 194 98 3 175 6 0 4
constraints. We then manually evaluated each mined constraint.
GitHub‚Äù 7 7 100 150 127 84.7 7 121 0 0 0
We used two datasets : 1) the AGORA [ 9 ] dataset, and 2) a self-
Marvel 28 23 82.1 115 55 47.8 13 35 6 0 4
OMDB 4 4 100 16 15 93.8 3 13 0 0 1 collected dataset (will be explained later). For overlapping analysis,
OMDB‚Äô 2 1 50 5 5 100 0 3 0 0 1
we grouped the invariants from AGORA into the groups for one
Spotify 21 21 100 41 41 100 11 26 3 2 5
specific variable, two variables, and so on. We then compared the
Spotify‚Äô 13 12 92.3 68 58 85.3 1 29 5 4 2
Spotify‚Äù 15 15 100 55 45 81.8 3 28 6 2 4
groups of invariants and constraints on the same set of variables.
Yelp 2 1 50 30 12 40 1 11 0 0 0
As evaluation metrics , we report the number of detected con-
YouTube 65 62 95.4 194 111 57.2 45 52 12 2 3
straints, the number of True Positives (TP), False Positives (FP), and
TOTAL 217 198 91.2 989 724 73.2 107 527 46 11 34
TP
the precision ùëÉ , with P = . Specifically, for AGORA, we reuse
Static: LLM-based, Dynamic: Execution-based (AGORA), +S: Static con- TP + FP
their experimental results (Total, TP, and P) on their dataset.
straint better, +D: Dynamic constraint better, Eq: Equivalent.
7.1.2 Results on the AGORA dataset. Table 1 shows the results
on the AGORA dataset with 11 API operations on 7 APIs. Our
LLM to follow predefined rules (similar to 5.1) to maintain consis-
LLM-based static method identified 217 constraints, with 198 true
tency in the generated code in different constraints.
positives, resulting in a precision of 91.2% . In contrast, AGORA
detected 724 true positives out of 989 invariants, resulting in a
6 Empirical Evaluation
precision of 73.2%. Notably, a variable has only one constraint,
For evaluation, we conduct several experiments, seeking to answer
while it can have multiple invariants from AGORA.
the following research questions:
RBCTest combines the constraint results from both LLM-based
RQ1. [Constraints Test Generation Accuracy] How well does
static method and the dynamic approach in AGORA. Thus, we
RBCTest perform in comparison with individual static and dynamic
also conducted an overlapping analysis between them. Overlapping
approaches in generating test cases from the mined constraints?
constraints are those applied to the same variables, and we compare
RQ2. [Constraints Mining Accuracy] How well does RBCTest
how well the constraints are detected. A constraint is considered
perform in detecting the onstraints for response bodies in the REST-
as better than one invariant or a group of invariants if it refers
ful API specification?
to a narrower set of values for the variable(s) while still adhering
RQ3. [Accuracy in Test Generation from the Correctly
to the variables‚Äô description. Similar definition is used for a better
Mined Constraints] How accurate is our approach in generating
invariant or group of invariants. They are equivalent if they cover
test cases for mined constraints?
exactly the same set of possible values for the variable(s).
RQ4. [Usefulness in Response Body Testing] How accurate
Our overlapping analysis results reveal that 107 constraints were
is our approach in detecting mismatches between the specification
uniquely detected by our LLM-based static method, while 527 were
and its working APIs?
exclusively identified by AGORA . We further investigated the invari-
RQ5. [Ablation Study] How well do Confirmation-Observation
ants detected only by AGORA and found that 319 of them pertained
and Semantic Verifier contribute to RBCTest ‚Äôs performance?
to variables lacking descriptions in API specification. This suggests
Data Collection. We curated a dataset from eight real-world ser- that these invariants were only detectable through the API execu-
vices, including GitLab and Stripe, comprising 59 endpoints and tions. These invariants primarily involved checks such as 1) whether
a variable is URL (33%), 2) a substring of another variable (32%), 3)
83 operations. These services were selected for several reasons: (1)
equal to another variable (13%), or 4) related to string length (7%),
They feature complex request-response structures across diverse
and (5) 15.6% covering other types.
business domains. (2) They have been widely used in API testing
Conversely, the constraints uniquely detected by our LLM-based
research, such as ‚ÄòCanada Holidays‚Äô [ 22 ], GitLab-services [ 14 , 18 , 22 ,
static method mainly occurred in scenarios where the API speci-
23 , 35 , 36 ], and ‚ÄòStripe‚Äô [ 22 , 28 , 30 ]. (3) Their active status allows API
fication provided variable descriptions, but the AGORA dataset‚Äôs
calls to collect real response data. (4) Their specifications vary in
documentation quality, enabling evaluation across different levels API responses did not include these variables (often optional fields).
of completeness. We selected API endpoints based on the presence AGORA‚Äôs dependency on the diversity of API responses at runtime
of parameter or response descriptions, excluding those without any limits its detection capability in such cases. For instance, in the
descriptions, as constraints could not be identified. Stripe, offering a YouTube API, there are 14 distinct rating schemas that appear only
test mode with limited endpoints, contains deeply nested response in specific request regions. If AGORA‚Äôs API calls do not cover all
schemas, often missing values for validation. To mitigate this, we regions, these schemas remain undetected.
included only Stripe endpoints without nested schemas, retaining 7
7.1.3 Result Analysis. A closer look at the cases where our LLM-
in total. We cover 3 REST methods: GET, PUT, and POST (Table 3).
based static method was superior reveals two main reasons for
its better performance. First, our LLM-based static method han-
dled more specific domains or ranges of values . For example, in the
Amadeus Hotel API, the roomQuantity value is specified as "an integer
between 1 and 9." Our LLM-based static method correctly identified
7

===== PAGE 8 =====
Submitted to a conference, Mar 15, 2025, XXX, XXX Hieu Huynh, Tri Le, Vu Nguyen, and Tien N. Nguyen
Table 2: RBCTest : Combining constraints and invariants Table 3: Constraints Test Generation on RBCTest Dataset
Overlapping Combination RBCTest
API-Op Static Dyn. Unq. S Unq. D Dist. API # Op. Methods Type GT
Cons. Incons. Total TP P% TP FP FN P% R% F1%
A.Hotel 39 61 20 34 54 11 8 109 73 67 C.Holiday 4 GET RP 24 16 0 8 100 66.7 80
GitHub 13 194 3 175 178 4 6 194 188 96.9 G.Branch 5 GET,POST RR 49 35 3 14 92.1 71.4 80.4
GitHub‚Äô 7 127 7 121 128 0 0 152 128 84.2 G.Commit 11 GET,POST,PUT RR 73 54 3 19 94.7 74 83.1
Marvel 23 55 13 35 48 4 6 113 58 51.3 G.Groups 14 GET,POST,PUT RR 85 61 3 24 95.3 71.8 81.9
OMDB 4 15 3 13 16 1 0 18 17 94.4 G.Issues 21 GET,POST,PUT RR 141 92 3 49 96.8 65.2 77.9
OMDB‚Äô 1 5 0 3 3 1 0 6 4 66.7 G.Project 15 GET,POST,PUT RR 144 110 11 34 90.9 76.4 83
Spotify 21 41 6 35 41 7 3 47 47 100 G.Repo. 3 GET,POST RR 44 33 3 11 91.7 75 82.5
Spotify‚Äô 12 58 1 29 30 6 5 50 41 82 RR 19 12 1 7 92.3 63.2 75
Stripe 10 GET,POST
Spotify" 15 45 1 11 12 6 6 53 43 81.1 RP 21 16 1 5 94.1 76.2 84.2
Yelp 1 4 1 11 12 0 0 31 12 38.7
Total 83 600 429 28 171 93.9 71.5 81.2
YouTube 62 111 45 52 97 5 12 151 114 75.5
Stdev 2.9 4.9 2.9
Total 198 724 107 527 634 45 46 924 725 78.5
Services: Canada Holidays, GitLab {Branch, Commit, Groups, Issues,
Unq: Uniquely detected, Dist: Distinct, (In)Cons: (In)Consistent.
Project, Repository}, and Stripe. # of Operations (No. Ops), # of ground
truth constraints (GT), Precision (P), and Recall (R). RR for Request-
Response Constraints, and RP for Response-Property Constraints.
this constraint and generated an appropriate test, whereas AGORA
provided a general invariant "Numeric," encompassing any inte-
Table 4: LLM-based Constraint Mining, Constraints Test Gen-
ger, float, etc. Similarly, in the Spotify API, AGORA expected the
eration, and Test Outcomes on RBCTest Dataset.
thumbnailHeight to be "one of (64, 300, 640)," based on the observed
Constraints Mining (RQ2) Test Gen. (RQ3) Test Out. (RQ4)
data at runtime, but our LLM-based static method correctly identi-
API Type
TP FP FN P% R% F1% N ‚úì P% ‚úì √ó ?
fied it as "image height in pixels," which implies any positive integer.
C.Holiday RP 16 0 8 100 66.7 80 16 16 100 12 0 4
Second, our LLM-based static method excelled in mining specific
G.Branch RR 36 2 13 94.7 73.5 82.8 36 35 97.2 33 2 0
G.Commit RR 55 2 18 96.5 75.3 84.6 55 54 98.2 40 8 6
constraints. For instance, in the Amadeus Hotel API, the sellingTotal
G.Groups RR 63 2 22 96.9 74.1 84 62 61 98.4 60 1 0
is defined as "= Total + margins + markup + totalFees - discounts."
G.Issues RR 93 2 48 97.9 66 78.8 93 92 98.9 80 7 5
While AGORA simply concluded that sellingTotal was numeric,
G.Project RR 110 11 34 90.9 76.4 83 110 110 100 102 0 8
G.Repo. RR 33 3 11 91.7 75 82.5 33 33 100 30 2 1
our LLM-based static method was able to be more specific in the
RR 12 1 7 92.3 63.2 75 12 12 100 7 0 5
Stripe
constraint mined from the specification. However, there were a few
RP 17 0 4 100 81 89.5 17 16 94.1 14 1 1
cases where AGORA outperformed our LLM-based static method. Total 435 23 165 95.7 72.4 82.2 434 429 98.8 378 21 30
Those cases often involve invariants verifying the format of vari-
For test outcomes: ‚úì (Matched), √ó (Mismatched), and ? (Unknown).
ables containing URL, and our LLM-based static method sometimes
treated URLs as mere strings without further validation.
As seen in Table 3, RBCTest via LLM-based static method (no
7.1.4 Combined method. From Table 2, we can see that combining
AGORA) identified 457 constraints, corresponding to 457 test cases
both static and dynamic approaches yield a better result than that of
generated by our LLM-based static method. This includes 28 false
individual method. In total, RBCTest has 107 constraints detected
positives and 171 missed constraints, yielding an overall precision
only by our static method, 527 detected only by AGORA. For cases
of 93.9% and a recall of 71.5% , with an F1 score of 81.2% . All
where both approaches detected constraints and invariants on the
standard deviations were below 5%, indicating consistently strong
same variables, in total, we identified 91 overlapping constraints
performance across different APIs. The inaccuracy mainly arises
and invariants. Among them, 46 cases are inconsistent. Inconsisten-
from the effects of the filter and verifier: in an effort to minimize
cies occur where an invariant extracted from execution data defines
the potential for erroneous outputs from the LLM, we restricted
a set of instances outside of the set defined by the respective con-
both the input to and the output from the model to ensure accuracy.
straints mined from the specification. The inconsistencies detected
This result is consistent with that in Section 7.1.2.
by RBCTest may reflect a coding bug or out-of-date specification.
Our analysis reveals that response-property constraints ‚Äîwhich
As seen in Table 2, RBCTest (the combined method) detects a
apply to a single response property‚Äîare typically straightforward,
total of 924 constraints, of which 725 are true positives, i.e., 78.5%
focusing on format or value range. They account for 32 of the
precision. RBCTest can identify constraints from both views while
429 detected constraints. However, in GitLab services, they were
maintaining sufficient precision. Despite of lower precision than the
difficult to detect due to sparse property descriptions. In contrast,
LLM-based method, RBCTest detects more true positive constraints
request-response constraints were more prevalent, as they involve
(725) in comparison to the individual methods, that is, the 198 true pos-
multiple variables, capturing complex relationships where a request
itive constraints from the LLM-based method and the 724 individual
parameter can influence multiple response properties.
invariants and 618 variable-based grouped invariants from AGORA .
7.2 Constraint Mining Accuracy (RQ2)
7.1.5 Results on the RBCTest dataset. For generalization, we re-
7.2.1 Methodology. In RQ1, we assess the entire process from the
peated the experiment on our collected dataset (Table 3), having 8
input of API specifications to the test generation. In this RQ2, we
APIs with 83 operations and ‚ÄòGET‚Äô, ‚ÄòPOST‚Äô, and ‚ÄòPUT‚Äô. We manually
focus on RBCTest ‚Äôs first component, Static Constraint Mining via
reviewed the API specifications and identified a set of 600 correct
LLMs. Thus, we did not run AGORA. We used the RBCTest dataset
constraints as the oracle. Due to some services requiring enterprise
subscriptions, we could not execute AGORA on this dataset.
8

===== PAGE 9 =====
Combining Static and Dynamic Approaches for Mining and Testing Constraints for RESTful API Testing Submitted to a conference, Mar 15, 2025, XXX, XXX
with the available API specifications. To decide the constraints‚Äô 7.3.2 Results. As shown in Table 4, RBCTest generated 434 test
correctness in natural language, we use the following rules: cases for 435 TP constraints (one test case was discarded by the
(1) Request-Response Property constraint : This type of con- verifier), including 401 for Request-Response constraints and 33
straint on a request parameter must correspond to a property in for Response Property constraints. It achieves a precision of 98.8%
the response data that reflects this constraint. across 8 services and confirmed 429 correct test cases. Examining
(2) Range-of-Value constraint : The type of constraint must services, 4/8 services achieved a precision of 100%, while the lowest
specify all possible values or provide a specific data range. precision was in the Stripe service, with a precision of 94.1%.
(3) Data Format constraint : This must describe the constraints Results indicate that RBCTest excels in generating code for
on data format or refer to widely used formats (e.g., ISO, Unix). Response Property constraints , which mainly involve format or value
range validation. In contrast, Request-Response constraints are more
7.2.2 Results. As seen in Table 4, 95.7% of the constraints identi-
error-prone due to the complex logic required to parse and verify
fied by our LLM-based static method are valid, although it missed
dependencies between request parameters and response properties.
165 out of 600 constraints noted in the ground truth. Results for all
Further analysis reveals that test generation errors primarily stem
metrics are notably higher than those of the entire process evalu-
from (1) missing descriptions and (2) ambiguous keywords.
ated in RQ1, as in RQ1‚Äôs experiment, we aim to account for both
Consider the ‚Äòget-/issues‚Äô endpoint in GitLab Issues, where a
the validity of constraints and the accuracy of the generated tests.
constraint exists between the request parameter ‚Äòdue_date‚Äô and a
As in Section 3, the detected constraints fall into two categories:
response property of the same name. The parameter is described as
Response Property constraints and Request-Response constraints.
‚ÄúAccepts: 0 (no due date), overdue, week, month, next_month,‚Äù while
Our LLM-based static method‚Äôs constraint mining for Response
the response property lacks a description. As a result, our tool gen-
Properties proves more effective than for Request-Response con-
erates test cases to validate the property against this constraint,
straints, achieving 100% precision and 73.3% recall compared to
even though ‚Äòdue_date‚Äô in the response is a date-time string, lead-
94.6% precision and 72.4% recall for Request-Response constraints.
ing to incorrect tests. Such errors are common in GitLab due to
Response Property constraints primarily define format and value
insufficient descriptions for response properties.
ranges, with clear descriptions like ‚Äúthree-letter currency code‚Äù or
In the same ‚Äòget-/issues‚Äô endpoint, the ‚Äòmilestone‚Äô parameter is
‚ÄúUnix epoch timestamp.‚Äù In contrast, Request-Response constraints
described as ‚ÄúThe milestone title. None lists all issues with no mile-
are more complex, requiring precise mappings between request
stone. Any lists all issues that have an assigned milestone.‚Äù The API
parameters and response properties. For example, in GitLab, ‚Äòsince‚Äô
filters returned issues based on ‚ÄòNone‚Äô or ‚ÄòAny‚Äô from the request pa-
and ‚Äòuntil‚Äô correctly map to ‚Äòcreated_at‚Äô to enforce range condi-
rameter. However, RBCTest misinterpreted ‚ÄòNone‚Äô as Python‚Äôs null ,
tions. However, most false positives arise from incorrect mappings,
leading to errors. This mistake in ‚Äòmilestone‚Äô propagated to 13 other
especially in GitLab, where response body descriptions are lack-
incorrect test cases due to its involvement in multiple operations.
ing. Without details, mappings rely on attribute names, leading to
errors‚Äîe.g., the ‚Äòavatar‚Äô parameter (for uploading images) is mistak-
7.4 Usefulness: Detecting Mismatches between
enly linked to ‚Äòavatar_url‚Äô in the response due to name similarity.
Constraints and Response Bodies (RQ4)
7.4.1 Methodology. We use the generated test cases to detect mis-
7.3 Test Generation Accuracy from Correctly
matches between the constraints in the API specification and the API
Mined Constraints (RQ3)
response bodies . With this goal, we did not run AGORA. For each
7.3.1 Methodology. While RQ1 evaluates the entire process from
API endpoint, we execute the API with multiple sets of request pa-
constraint mining to test generation, this RQ3 focuses only on evalu-
rameters. For each API execution, we collect 1) request information
ating the test generation component for the constraints correctly de-
(i.e., what is expected from the API call), and 2) response data (i.e.,
tected by RBCTest via our LLM-based mining on the RBCTest dataset
the actual response ). Response data contains multiple properties,
(thus, we did not run AGORA) . We performed test generation from
each attached with constraints and generated constraint test cases.
those correct constraints. The generated test cases are manually
We ran the test cases associated with the response data to collect the
evaluated if they correctly verify the associated constraints.
outcomes. We also ran the inputs used in AGORA to verify against
We used the following rules to check if a test case is correct:
our test cases of correctly generated constraints. If the result is
(1) Test Input: The generated test case is correct if it correctly
false, we report a mismatch. Otherwise, it is a match.
receives two inputs for Request-Response constraints: 1) requested
information and 2) response data, and one input for Response Prop- 7.4.2 Results. We performed test runs on 429 correctly generated
erty constraints: response data. tests from RQ3 (Table 4). Our test results indicate 378 ‚Äòmatched‚Äô
(2) Constraint Handling: The generated test case must cover response bodies (i.e., consistent with the specification), 21 ‚Äòmis-
all conditions in the constraint. matched‚Äô, and 30 ‚Äòunknown‚Äô. Out of 434 correctly mined constraints,
(3) Test Output: The test must return: 378 were verified by the tests, meaning that 87.1% of the constraints
i) 0 ( unknown ) if lacking of sufficient data for condition checking are met by the actual execution of the SUTs. Our tool detected
(e.g., empty or null value). 21 mismatches, revealing inconsistencies between specifica-
ii) 1 ( matched ) if the provided input satisfies the constraint. tions and execution of the APIs . An ‚Äòunknown‚Äô occurs when a
iii) -1 ( mismatched ) if the provided input does not satisfy it. property is absent in the response body due to its optional nature.
We only consider the set of test cases derived from valid con-
straints as identified in RQ2 (denoted as ùëÅ in Table 4).
9

===== PAGE 10 =====
Submitted to a conference, Mar 15, 2025, XXX, XXX Hieu Huynh, Tri Le, Vu Nguyen, and Tien N. Nguyen
Table 5: Contribution of components in RBCTest (RQ5)
The semantic verifier‚Äôs goal is to remove the invalid constraints
Constraints Mining
to improve precision. We currently used a simple verifying mecha-
Variant
TP FP FN P R F1
nism via examples in API specification (Section 2.3). We removed
RBCTest 435 22 165 95.2 72.5 82.3
the semantic verifier and ran the static component on two datasets.
‚àí
RBCTest 197 127 403 60.8 32.8 42.6
In the RBCTest dataset with 89 examples, one example invalidated
one detected constraint. In the AGORA dataset, with 223 exam-
ples over 11 API operations, 6 examples were able to invalidate
7.4.3 Analysis. We revealed the following root causes of these mis-
6 false-positive constraints. Overall, the verifier accurately con-
matches: (1) incompatible data formats, (2) not-explicitly-described
firmed all valid constraints and successfully eliminated 7 incorrect
nullable properties, and (3) inter-parameter request dependencies.
constraints, thus achieving higher precision (89.0% increasing to
(1) Incompatible Data Formats : Our tool detected 21 con-
91.2%) while maintaining recall (72.5%). These results show that
straint mismatches , mainly from GitLab services. For instance, the
more examples in the API are useful to invalidate more incorrect
constraint "date will be returned in ISO 8601 format YYYY-MM-DDTHH:MM"
constraints and confirm the correct ones. Moreover, other types
appears in GitLab operations. However, the actual data format is
of semantic verifier can be integrated into our framework such as
"2012-09-20T08:50:22.000Z" , which includes a decimal part for sec-
constraint solvers, SMT solvers, domain-specific checkers (valid zip
onds, leading to an inconsistency. Interestingly, we found that three
code, phone number format, or valid date checkers), etc.
instances of this type of inconsistency were reported as the is-
sues on the GitLab forum [ 1 , 3 , 4 ]. This is anecdotal evidence on
8 Threats to Validity
RBCTest ‚Äôs usefulness in detecting real-world issues .
1. Internal Validity. LLM hallucinations might lead to unexpected
(2) Not-Explicitly-Described, Nullable Properties : This issue
outcomes. To mitigate this, we incorporated (1) a semantic verifier
is common in GitLab, where some response properties are nullable
(Figure 8) and (2) observation-confirmation prompting (Figure 3),
but lack descriptions in the specification, leading to parsing errors.
which enhance validation through external API specifications and
Notably, these issues have been reported on the GitLab forum [2].
internal consistency checks. Our ablation study confirmed that
(3) Inter-Parameter Request Dependencies : This was found
these methods have a positive impact on performance. We seg-
in only one case. For the operation ‚ÄôGET/groups‚Äô from GitLab Group,
mented specifications into small sections (Figure 3) to reduce con-
there is a constraint on the request parameter "order_by" affecting
text window, minimizing hallucinations and improving accuracy.
the "name" property in the response data. This logic dictates that the
2. External Validity. Our dataset may not fully represent the API
array of groups in the API response should be sorted according to
landscape, affecting generalizability. Outcomes could vary with
the "order_by" parameter. RBCTest checked only one-to-one con-
different datasets, particularly those with unique constraints. Gen-
ditions among parameters, whereas the sorting order depends on
erated test cases may miss general constraints or edge cases beyond
both "order_by" and "sort" parameters (specifying the sort direction).
what is explicitly documented. Implicit constraints not in API spec-
As a result, this resulted in a detected mismatch.
ifications might also be valid but unaccounted for. External tools
may introduce inaccuracies, potentially affecting the results.
7.5 Ablation Study (RQ5)
In this experiment, we first built a variant that does not contain both
9 Related Work
observation-confirmation and semantic verifier components. To
Recent surveys on API testing [ 15 , 17 , 21 , 25 , 26 , 28 , 32 ] reveal a
learn more the static method, the dynamic component was excluded.
trend towards automation adoption. AI/ML are used to enhance var-
We modify the prompt in Figure 4 as follows for constraint mining.
ious aspects of API testing including of generating test cases [ 14 , 27 ,
Instead of providing GPT-4-Turbo with observations derived from
33 ], realistic test inputs [ 8 ], and identify defects early in the devel-
another prompt, we feed the description of the parameters and
opment [ 11 ‚Äì 13 , 24 , 31 , 37 ‚Äì 40 ]. AGORA is a dynamic approach [ 9 ].
response schema as in the API specification. This prompt replaces
The advances of LLMs have impacted API testing. [ 22 ] leverages
Blocks 1 - 5 from Figure 3. After providing data on a property,
the language capabilities of GPT to fully automate API testing using
we instruct the LLM to decide if the given property contains a
only an input OpenAPI specification. It builds a dependency graph
constraint and if there is enough to verify it. This modification
among API operations, generating test scripts and inputs. Moreover,
merges steps 6 and 7 into a single step. The output is yes or no .
[ 20 ] applied GPT to augment specifications, enriching them with
As seen in Table 5, the elimination of observation-confirmation
explanations of rules and example inputs. Before the era of LLMs,
prompting and semantic verifier affects the outcomes, as evidenced
ARTE [ 8 ] aimed to generate test inputs for API testing, employing
by a reduction of 238 correct constraints. Concurrently, there is an
NLP. Morest [ 24 ] introduced a model-based RESTful API testing
‚àí
increase in the number of false positives. The F1-score of RBCTest
method using a dynamically updating RESTful-service Property
is reduced by half. False positives frequently arise when the model
Graph, showing improvements in code coverage and bug detection.
mis-mapping the request parameters with the response properties
RESTler [ 14 ] is a stateful fuzzing tool of REST APIs, which ana-
based on their names. Such mistakes are less common in RBCTest,
lyzed specifications, inferred dependencies among request types,
where the observation prompt is used to enhance the property be-
and dynamically generated tests guided by feedback from service
fore it is processed by the confirmation prompt. This result confirms
responses. Similarly, RestTestGen [ 33 ] applied specifications for au-
our key contribution of our LLM prompting strategy .
tomatically generating test cases, checking both response status and
response data schemas. NLPtoREST [ 19 ] used the NLP technique
10

===== PAGE 11 =====
Combining Static and Dynamic Approaches for Mining and Testing Constraints for RESTful API Testing Submitted to a conference, Mar 15, 2025, XXX, XXX
Word2Vec [ 29 ] to extract rules from human-readable descriptions References
in a specification, enhance, and add them back to the specification. [1] 2024. GitLab Issue 19667. https://gitlab.com/gitlab-org/gitlab/-/issues/19667
[2] 2024. GitLab Issue 348376. https://gitlab.com/gitlab-org/gitlab/-/issues/348376
[3] 2024. GitLab Issue 349664. https://gitlab.com/gitlab-org/gitlab/-/issues/349664
10 Conclusion
[4] 2024. GitLab Issue 410638. https://gitlab.com/gitlab-org/gitlab/-/issues/410638
[5] 2024. Katalon Studio Automation Testing Tool. https://katalon.com/
Novelty . This paper presents RBCTest , an approach to mine the
[6] 2024. Postman API Testing Tool. https://www.postman.com/
constraints on API response bodies and automatically generate test
[7] 2025. RBCTest. https://github.com/api-rbtest/RBCTest
[8] Juan C Alonso, Alberto Martin-Lopez, Sergio Segura, Jose Maria Garcia, and
cases to validate them. Our key findings include 1) our LLM-based
Antonio Ruiz-Cortes. 2022. ARTE: Automated Generation of Realistic Test Inputs
static method with Observation-Confirmation prompting
for Web APIs. IEEE Transactions on Software Engineering 49, 1 (2022), 348‚Äì363.
achieves high precision in constraint mining of 91.2% ; and 2) when [9] Juan C. Alonso, Sergio Segura, and Antonio Ruiz-Cort√©s. 2023. AGORA: Auto-
mated Generation of Test Oracles for REST APIs. In Proceedings of the 32nd ACM
combining static and dynamic approaches , RBCTest takes ad-
SIGSOFT International Symposium on Software Testing and Analysis (Seattle, WA,
vantage of both LLM‚Äôs capabilities to comprehend natural language
USA) (ISSTA 2023) . Association for Computing Machinery, New York, NY, USA,
1018‚Äì1030. https://doi.org/10.1145/3597926.3598114
in API specifications, when available, and API execution informa-
[10] Andrea Arcuri. 2019. RESTful API automated test case generation with EvoMaster.
tion to detect constraints on response bodies. With the combination,
ACM Transactions on Software Engineering and Methodology (TOSEM) 28, 1 (2019),
RBCTest detects more true positive constraints, while reducing a
1‚Äì37.
[11] Andrea Arcuri. 2020. Automated black-and white-box testing of restful apis with
bit precision. RBCTest -generated test cases was able to detect 21
evomaster. IEEE Software 38, 3 (2020), 72‚Äì78.
mismatches in real-world APIs , four of which were confirmed
[12] Andrea Arcuri and Juan P Galeotti. 2020. Handling SQL databases in automated
system test generation. ACM Transactions on Software Engineering and Methodol-
by developers in their development forums.
ogy (TOSEM) 29, 4 (2020), 1‚Äì31.
Practical Impact . RBCTest allows teams to test API services of
[13] Andrea Arcuri and Juan P Galeotti. 2021. Enhancing search-based testing with
SUTs in both the development and evolution stages. By using API
testability transformations for existing APIs. ACM Transactions on Software
Engineering and Methodology (TOSEM) 31, 1 (2021), 1‚Äì34.
specifications, RBCTest supports applicable use cases where devel-
[14] Vaggelis Atlidakis, Patrice Godefroid, and Marina Polishchuk. 2019. Restler:
opers need to investigate and test third-party APIs with publicly
Stateful rest api fuzzing. In 2019 IEEE/ACM 41st International Conference on
available specifications before using them for their applications. Software Engineering (ICSE) . IEEE, 748‚Äì758.
[15] Adeel Ehsan, Mohammed Ahmad ME Abuhaliqa, Cagatay Catal, and Deepti
Data Availability. Our data and code is publicly available [7].
Mishra. 2022. RESTful API testing methodologies: Rationale, challenges, and
solution directions. Applied Sciences 12, 9 (2022), 4369.
[16] Michael D. Ernst, Jeff H. Perkins, Philip J. Guo, Stephen McCamant, Carlos
Pacheco, Matthew S. Tschantz, and Chen Xiao. 2007. The Daikon system for
dynamic detection of likely invariants. Sci. Comput. Program. 69, 1‚Äì3 (dec 2007),
35‚Äì45. https://doi.org/10.1016/j.scico.2007.01.015
[17] Amid Golmohammadi, Man Zhang, and Andrea Arcuri. 2022. Testing RESTful
APIs: A Survey. ACM Transactions on Software Engineering and Methodology
(2022).
[18] Stefan Karlsson, Adnan ƒåau≈°eviƒá, and Daniel Sundmark. 2020. QuickREST:
Property-based test generation of OpenAPI-described RESTful APIs. In 2020
IEEE 13th International Conference on Software Testing, Validation and Verification
(ICST) . IEEE, 131‚Äì141.
[19] Myeongsoo Kim, Davide Corradini, Saurabh Sinha, Alessandro Orso, Michele
Pasqua, Rachel Tzoref-Brill, and Mariano Ceccato. 2023. Enhancing REST API
Testing with NLP Techniques. In Proceedings of the 32nd ACM SIGSOFT Interna-
tional Symposium on Software Testing and Analysis . 1232‚Äì1243.
[20] Myeongsoo Kim, Tyler Stennett, Dhruv Shah, Saurabh Sinha, and Alessandro
Orso. 2024. Leveraging large language models to improve REST API testing.
In Proceedings of the 2024 ACM/IEEE 44th International Conference on Software
Engineering: New Ideas and Emerging Results . 37‚Äì41.
[21] Myeongsoo Kim, Qi Xin, Saurabh Sinha, and Alessandro Orso. 2022. Automated
test generation for rest apis: No time to rest yet. In Proceedings of the 31st ACM
SIGSOFT International Symposium on Software Testing and Analysis . 289‚Äì301.
[22] Tri Le, Thien Tran, Duy Cao, Vy Le, Vu Nguyen, and Tien N. Nguyen. 2024. KAT:
Dependency-aware Automated API Testing with Large Language Models. In 2024
IEEE Conference on Software Testing, Verification and Validation (ICST) . IEEE.
[23] Jiaxian Lin, Tianyu Li, Yang Chen, Guangsheng Wei, Jiadong Lin, Sen Zhang,
and Hui Xu. 2022. foREST: A Tree-based Approach for Fuzzing RESTful APIs.
arXiv preprint arXiv:2203.02906 (2022).
[24] Yi Liu, Yuekang Li, Gelei Deng, Yang Liu, Ruiyuan Wan, Runchao Wu, Dandan
Ji, Shiheng Xu, and Minli Bao. 2022. Morest: model-based RESTful API testing
with execution feedback. In Proceedings of the 44th International Conference on
Software Engineering . 1406‚Äì1417.
[25] Bogdan Marculescu, Man Zhang, and Andrea Arcuri. 2022. On the faults found in
rest apis by automated test generation. ACM Transactions on Software Engineering
and Methodology (TOSEM) 31, 3 (2022), 1‚Äì43.
[26] Alberto Martin-Lopez, Andrea Arcuri, Sergio Segura, and Antonio Ruiz-Cort√©s.
2021. Black-box and white-box test case generation for RESTful APIs: Enemies
or allies?. In 2021 IEEE 32nd International Symposium on Software Reliability
Engineering (ISSRE) . IEEE, 231‚Äì241.
[27] Alberto Martin-Lopez, Sergio Segura, and Antonio Ruiz-Cort√©s. 2021. RESTest:
Automated Black-Box Testing of RESTful Web APIs. In Proceedings of the 30th
ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA
‚Äô21) . Association for Computing Machinery.
11

===== PAGE 12 =====
Submitted to a conference, Mar 15, 2025, XXX, XXX Hieu Huynh, Tri Le, Vu Nguyen, and Tien N. Nguyen
[28] Alberto Martin-Lopez, Sergio Segura, and Antonio Ruiz-Cort√©s. 2022. Online Large Language Models. CoRR abs/2201.11903 (2022). arXiv:2201.11903 https:
testing of RESTful APIs: Promises and challenges. In Proceedings of the 30th //arxiv.org/abs/2201.11903
ACM Joint European Software Engineering Conference and Symposium on the [35] Huayao Wu, Lixin Xu, Xintao Niu, and Changhai Nie. 2022. Combinatorial testing
Foundations of Software Engineering . 408‚Äì420. of restful apis. In Proceedings of the 44th International Conference on Software
[29] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Engineering . 426‚Äì437.
Estimation of Word Representations in Vector Space. arXiv:1301.3781 [cs.CL] [36] Koji Yamamoto. 2021. Efficient penetration of API sequences to test stateful
https://arxiv.org/abs/1301.3781 RESTful services. In 2021 IEEE International Conference on Web Services (ICWS) .
[30] A Giuliano Mirabella, Alberto Martin-Lopez, Sergio Segura, Luis Valencia- IEEE, 734‚Äì740.
Cabrera, and Antonio Ruiz-Cort√©s. 2021. Deep learning-based prediction of [37] Man Zhang and Andrea Arcuri. 2021. Adaptive hypermutation for search-based
test input validity for restful apis. In 2021 IEEE/ACM Third International Workshop system test generation: A study on rest apis with evomaster. ACM Transactions
on Deep Learning for Testing and Testing for Deep Learning (DeepTest) . IEEE, 9‚Äì16. on Software Engineering and Methodology (TOSEM) 31, 1 (2021), 1‚Äì52.
[31] Omur Sahin and Bahriye Akay. 2021. A discrete dynamic artificial bee colony [38] Man Zhang and Andrea Arcuri. 2021. Enhancing resource-based test case gener-
with hyper-scout for RESTful web service API test suite generation. Applied Soft ation for RESTful APIs with SQL handling. In International Symposium on Search
Computing 104 (2021), 107246. Based Software Engineering . Springer, 103‚Äì117.
[32] Abhinav Sharma, M Revathi, et al . 2018. Automated API testing. In 2018 3rd [39] Man Zhang, Bogdan Marculescu, and Andrea Arcuri. 2019. Resource-based
International Conference on Inventive Computation Technologies (ICICT) . IEEE, test case generation for restful web services. In Proceedings of the genetic and
788‚Äì791. evolutionary computation conference . 1426‚Äì1434.
[33] Emanuele Viglianisi, Michael Dallago, and Mariano Ceccato. 2020. Resttest- [40] Man Zhang, Bogdan Marculescu, and Andrea Arcuri. 2021. Resource and depen-
gen: automated black-box testing of restful apis. In 2020 IEEE 13th International dency based test case generation for RESTful Web services. Empirical Software
Conference on Software Testing, Validation and Verification (ICST) . IEEE, 142‚Äì152. Engineering 26, 4 (2021), 76.
[34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc
Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in
12